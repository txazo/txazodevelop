
1. 创建Hadoop用户
groupadd hadoop
useradd -g hadoop hadoop
passwd hadoop

1. 修改机器名称
Master: /etc/sysconfig/network
HOSTNAME=master
Slave: /etc/sysconfig/network
HOSTNAME=slave

2. 配置Hosts(Master Slave)
vi /etc/hosts
112.124.6.220  master
120.24.102.136 slave

3. SSH无密码验证配置
yum install ssh rsync
service sshd restart

cd /home/hadoop
ssh-keygen -t rsa
cd ~/.ssh
cat id_rsa.pub >> authorized_keys
chmod 600 authorized_keys

Master
root: vi /etc/ssh/sshd_config
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys
service sshd restart

scp ~/.ssh/id_rsa.pub hadoop@120.24.102.136:~/
mkdir ~/.ssh
chmod 700 ~/.ssh
cat ~/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

Slave
root: vi /etc/ssh/sshd_config
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys
service sshd restart

Test
ssh 120.24.102.136

Slave
cd /home/hadoop
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

scp ~/.ssh/id_rsa.pub hadoop@112.124.6.220:~/
cat ~/id_rsa.pub >> ~/.ssh/authorized_keys

Test
ssh 112.124.6.220

1. 安装JDK

1. 安装Hadoop
wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
tar -zxvf hadoop-2.6.0.tar.gz
mv hadoop-2.6.0 /usr/local/hadoop
chown -R hadoop:hadoop hadoop

vi /etc/profile
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
source /etc/profile

su hadoop
mkdir /usr/local/hadoop/tmp

1. 配置Hadoop
1) hadoop-env.sh
   yarn-env.sh
export JAVA_HOME=/usr/java/latest

2) core-site.xml
<configuration>
	<property>
	    <name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
	<property>
	    <name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop/tmp</value>
	</property>
	<property>
	    <name>io.file.buffer.size</name>
		<value>4096</value>
	</property>
</configuration>

3) hdfs-site.xml
<configuration>
	<property>
	    <name>dfs.nameservices</name>
		<value>hadoop-cluster</value>
	</property>
	<property>
	    <name>dfs.namenode.secondary.http-address</name>
		<value>master:50090</value>
	</property>
	<property>
	    <name>dfs.namenode.name.dir</name>
		<value>file:///usr/local/hadoop/dfs/name</value>
	</property>
	<property>
	    <name>dfs.namenode.data.dir</name>
		<value>file:///usr/local/hadoop/dfs/data</value>
	</property>
    <property>
	    <name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
	    <name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
</configuration>

4) mapred-site.xml
<configuration>
	<property>
	    <name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
    <property>
	    <name>mapreduce.jobtracker.http.address</name>
		<value>master:50030</value>
	</property>
	<property>
	    <name>mapreduce.jobhistory.address</name>
		<value>master:10020</value>
	</property>
	<property>
	    <name>mapreduce.jobhistory.webapp.address</name>
		<value>master:19888</value>
	</property>
</configuration>

5) yarn-site.xml
<configuration>
	<property>
	    <name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
    <property>
	    <name>yarn.resourcemanager.address</name>
		<value>master:8032</value>
	</property>
	<property>
	    <name>yarn.resourcemanager.scheduler.address</name>
		<value>master:8030</value>
	</property>
	<property>
	    <name>yarn.resourcemanager.resource-tracker.address</name>
		<value>master:8031</value>
	</property>
	<property>
	    <name>yarn.resourcemanager.admin.address</name>
		<value>master:8033</value>
	</property>
	<property>
	    <name>yarn.resourcemanager.webapp.address</name>
		<value>master:8088</value>
	</property>
</configuration>

5) slaves
slave

6) 启动及验证
Master: bin/hdfs namenode -format

7) 启动Hadoop
sbin/start-dfs.sh
sbin/start-yarn.sh

8) 停止Hadoop
/sbin/stop-dfs.sh
/sbin/stop-yarn.sh

9) 验证
jps

9) 浏览器访问
http://112.124.6.220:50070
http://112.124.6.220:8088

scp -r /usr/local/hadoop root@slave:/usr/local